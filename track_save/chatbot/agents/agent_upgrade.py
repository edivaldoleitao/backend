from langchain.prompts.chat import (
    ChatPromptTemplate,
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate,
)
from langchain.chains import LLMChain
from langchain.chat_models import ChatOpenAI
import pandas as pd
import json
from dotenv import load_dotenv
import os
import re
from pathlib import Path
import requests

env_path = Path(__file__).resolve().parent.parent / '.envs' / '.local' / '.api_key_gpt'
load_dotenv(dotenv_path=env_path)
API_SEARCH_URL = "http://localhost:8000/api/search/"

from django.apps import apps


def generate_schema_string(app_label: str):
    schema_lines = []
    models = apps.get_app_config(app_label).get_models()

    for model in models:
        schema_lines.append(f"Tabela {model.__name__}:")
        for field in model._meta.fields:
            if (
                field.name == "id"
            ):  # Se quiser omitir o id, pode remover esta verifica√ß√£o
                continue
            schema_lines.append(f" - {field.name}: {field.get_internal_type()}")
        schema_lines.append("")  # Linha em branco entre models

    return "\n".join(schema_lines)


schema_str = generate_schema_string("api")

upgrade_prompt = ChatPromptTemplate.from_messages([
    SystemMessagePromptTemplate.from_template(
        "Voc√™ √© um especialista em hardware de computadores. Seu papel √©, dado o setup atual do usu√°rio, sua descri√ß√£o de problema "
        "e o schema do banco de dados, gerar upgrades compat√≠veis e superiores no formato JSON.\n\n"
        "Aqui est√° o schema do banco:\n{schema}\n\n"
        "Agora, considere o setup atual do usu√°rio e a descri√ß√£o do problema:\n"
        "{setup_json}\n{descricao_dor}\n\n"
        "Gere um JSON no seguinte formato EXATO:\n\n"
        "{{\n"
        '  "searches": [\n'
        "    {{\n"
        '      "model_name": "NomeDoModel",\n'
        '      "columns": ["coluna1", "coluna2"],\n'
        '      "search_values": ["valor1", "valor2"]\n'
        "    }}\n"
        "  ]\n"
        "}}\n\n"
        "Inclua apenas colunas e valores relevantes. Retorne **apenas o JSON**, sem explica√ß√µes adicionais."
        "Nunca use operadores como '>=', apenas indique valores m√≠nimos reais. Exemplo correto: 'vram': '8GB'. "
    ),
    HumanMessagePromptTemplate.from_template("{setup_json}\n{descricao_dor}")
])


recommendation_prompt = ChatPromptTemplate.from_messages([
    SystemMessagePromptTemplate.from_template(
        "Voc√™ √© TrackBot, um consultor especializado em tecnologia. Seu papel √© analisar as especifica√ß√µes atuais do usu√°rio, "
        "os upgrades sugeridos e os produtos dispon√≠veis no banco de dados.\n\n"
        "**Aten√ß√£o:**\n"
        "- S√≥ pode recomendar produtos que est√£o listados no banco de dados abaixo.\n"
        "- Se o banco estiver vazio, **n√£o invente produtos.**\n"
        "- Informe educadamente que n√£o foram encontrados produtos dispon√≠veis.\n\n"
        "Especifica√ß√µes atuais do usu√°rio:\n{setup_json}\n\n"
        "Upgrades sugeridos:\n{upgrade_specs}\n\n"
        "Produtos encontrados no banco:\n{products_json}\n\n"
        "Gere uma resposta explicando por que os produtos resolvem o problema, citando nome, pre√ßo e link.\n"
        "Se n√£o houver produtos, informe que n√£o h√° produtos dispon√≠veis."
    ),
    HumanMessagePromptTemplate.from_template("{setup_json}\n{upgrade_specs}\n{products_json}")
])


def processar_upgrade(setup_json: dict, descricao_dor: str, dfs: dict):
    llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0)

    print(json.dumps(setup_json, indent=2))
    print(descricao_dor)

    # 1. Gerar o JSON completo com upgrade_prompt
    upgrade_chain = LLMChain(llm=llm, prompt=upgrade_prompt)
    upgrade_specs_json = upgrade_chain.run({
        "schema": schema_str,
        "setup_json": json.dumps(setup_json),
        "descricao_dor": descricao_dor,
    })

    print("\nüìÑ JSON gerado pelo upgrade_prompt:")
    print(upgrade_specs_json)

    # 2. Limpar Markdown / formata√ß√£o (caso o LLM retorne com blocos de c√≥digo)

    match = re.search(r"({.*})", upgrade_specs_json, re.DOTALL)
    if match:
        upgrade_specs_json = match.group(1)

    # 3. Interpretar o JSON
    try:
        upgrade_specs = json.loads(upgrade_specs_json)
    except json.JSONDecodeError:
        return {"error": "Erro ao interpretar o JSON gerado pelo LLM."}

    print("\nüîé JSON pronto para API:")
    print(json.dumps(upgrade_specs, indent=2))

    # 4. Requisi√ß√£o para a API
    try:
        response = requests.post(API_SEARCH_URL, json=upgrade_specs)
        response.raise_for_status()
        search_results = response.json().get("results", [])
    except Exception as e:
        return {"error": f"Erro ao consultar API: {e!s} {upgrade_specs}"}

    print("\n‚úÖ Produtos retornados da API:")
    print(json.dumps(search_results, indent=2))

    # 5. Gerar recomenda√ß√£o final
    rec_chain = LLMChain(llm=llm, prompt=recommendation_prompt)
    recommendation = rec_chain.run({
        "setup_json": json.dumps(setup_json, indent=2),
        "upgrade_specs": json.dumps(upgrade_specs, indent=2),
        "products_json": json.dumps(search_results, indent=2)
    })

    print("\nüí¨ Recomenda√ß√£o final:")
    print(recommendation)

    return {
        "upgrade_specs": upgrade_specs,
        "produtos_encontrados": search_results,
        "resposta": recommendation
    }
